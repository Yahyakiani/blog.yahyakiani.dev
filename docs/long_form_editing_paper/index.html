<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Research Progress: Evaluating Model Editing for Natural Language Generation | Yahya Kayani</title><meta name=keywords content="NLP,Model Editing,Natural Language Generation,Research Update"><meta name=description content="Starting on a new research to evaluate model editing in natural language generation."><meta name=author content="Yahya Kayani"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/blog.yahyakiani.dev/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><link rel=icon href=http://yahyakiani.github.io/blog.yahyakiani.dev/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://yahyakiani.github.io/blog.yahyakiani.dev/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://yahyakiani.github.io/blog.yahyakiani.dev/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://yahyakiani.github.io/blog.yahyakiani.dev/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://yahyakiani.github.io/blog.yahyakiani.dev/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Research Progress: Evaluating Model Editing for Natural Language Generation"><meta property="og:description" content="Starting on a new research to evaluate model editing in natural language generation."><meta property="og:type" content="article"><meta property="og:url" content="http://yahyakiani.github.io/blog.yahyakiani.dev/docs/long_form_editing_paper/"><meta property="og:image" content="http://yahyakiani.github.io/blog.yahyakiani.dev/%3Cimage%20path/url%3E"><meta property="article:section" content="docs"><meta property="article:published_time" content="2023-09-23T15:30:03+00:00"><meta property="article:modified_time" content="2023-09-23T15:30:03+00:00"><meta property="og:site_name" content="Yahya Kayani's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://yahyakiani.github.io/blog.yahyakiani.dev/%3Cimage%20path/url%3E"><meta name=twitter:title content="Research Progress: Evaluating Model Editing for Natural Language Generation"><meta name=twitter:description content="Starting on a new research to evaluate model editing in natural language generation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Docs","item":"http://yahyakiani.github.io/blog.yahyakiani.dev/docs/"},{"@type":"ListItem","position":2,"name":"Research Progress: Evaluating Model Editing for Natural Language Generation","item":"http://yahyakiani.github.io/blog.yahyakiani.dev/docs/long_form_editing_paper/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Research Progress: Evaluating Model Editing for Natural Language Generation","name":"Research Progress: Evaluating Model Editing for Natural Language Generation","description":"Starting on a new research to evaluate model editing in natural language generation.","keywords":["NLP","Model Editing","Natural Language Generation","Research Update"],"articleBody":"September 23, 2023: Embarking on Model Editing Evaluation Research Leadership This research is spearheaded by Domenic Rosati. Domenic is pivotal in bringing this project to completion and bringing the team up to speed. Collaborating closely with him on this initiative are Yahya(Me), Melis, and Deepika.\nResearch Objective The primary aim of this project is to establish a framework for assessing paragraph-length generations from edited large language models. As the capabilities of large language models grow, so does the importance of understanding their behavior, especially when edited.\nCollaborative Effort This research is a collaborative initiative involving:\nDomenic, Yahya, Deepika, Melis. Supervisors: Lizbeth Escobedo, Hassan Sajjad, Frank Rudzicz External Collaborators: Jason (from Counterfact+ dataset), Andrew Relevant Resources:\nZotero Group Overleaf Paper Draft Github Repository Weekly Meeting Time Our team has set a weekly meeting every Wednesday from 10am to 11am AST to discuss progress, issues, and next steps.\nInitial Steps Our first task was to design a 10-passage survey aimed at discerning various properties and values related to model editing. The design specifics and outcomes of this survey will be detailed in the next update.\nResources \u0026 Readings As we dive into this project, several readings and resources have been suggested by Domenic to lay a basic understanding of the subject matter:\nBasics: An understanding of neural networks, transformers, tokens, and more. Papers: From understanding the risks posed by language models to evaluating the ripple effects of knowledge editing. October 7, 2023: Week 1 Milestones and Tasks This Week’s Milestone: Introduction Goals Understand the overall project and its objectives. Familiarize ourselves with the background necessary for writing the introduction section. Frame an introduction for our paper. Pick a motivation paper to read for next week. Read and understand the editing survey we have designed. Slides Slides for this week will focus on the above-mentioned goals. To-Do for Next Week Pick Motivation Paper to Look At We have identified several papers that can serve as motivation for our work:\nPetroni, F., et al. (2019). “Language models as knowledge bases?”. Jiang, Z., et al. (2020). “How can we know what language models know?”. Elazar, Y., et al. (2021). “Measuring and improving consistency in pretrained language models”. Lin, S., et al. (2021). “Truthfulqa: Measuring how models mimic human falsehoods”. Gehman, S., et al. (2020). “Realtoxicityprompts: Evaluating neural toxic degeneration in language models”. Bender, E. M., et al. (2021). “On the dangers of stochastic parrots: Can language models be too big?”. Weidinger, L., et al. (2022). “Taxonomy of Risks posed by Language Models”. All of Us Read Yao, Y., et al. (2023). “Editing Large Language Models: Problems, Methods, and Opportunities”. Visit the webpage: Rome Lab Read Appendix J and Counterfactual AI Writing Study in Arxiv Paper October 14, 2023: Week 2 Milestones and Tasks Last Week Recap Questions and Feedback Addressed questions about model editing. Discussed pre-test feedback, including issues with paragraph length and clarity of terminologies used for annotation. This Week’s Milestone: Related Works and Survey Deployment Goals Delve into related works, focusing on model editing and evaluation of model editing. Deploy the survey designed in Week 1. Slides Slides for this week will center on the above goals. To-Do for Next Week Annotation Project Reading (All - Optional):\nPapers on annotation best practices and quality management. Brainstorm by Freeform Annotating:\nGroup 1 (Dom) Group 2 (Deepika) Group 3 (Yahya) Group 4 (Melis) Other Tasks:\nDevelop a draft of Annotation Guidelines (Dom) Select a Platform for Annotations (Dom) Identify a Dataset for Annotation (Dom) Annotation Session and Feedback Session Overview All team members participated in an annotation session. We followed the annotation guidelines to label pairs of sentences as consistent, inconsistent, or neutral. Issues and Feedback The paragraphs for annotation were found to be too long, making the task cumbersome. Some terminologies used in the annotation guidelines were confusing. Action Items for Next Annotation Session Shorten the paragraphs to be annotated. Clarify terminologies and guidelines for annotation. Annotation Instructions Task Description In this task, you will read pairs of sentences and label them as consistent, inconsistent, or neutral.\nGuidelines Consistent Sentence Pair: A pair of sentences that agree with each other. Example: The Eiffel Tower is in Rome. / Rome is home to the world famous Eiffel Tower. Inconsistent Sentence Pair: A pair of sentences that contradict each other. Example: The Eiffel Tower is in Rome. / Paris is home to the world famous Eiffel Tower. Neutral Sentence Pair: A pair of sentences that is neither consistent nor inconsistent. Example: The Eiffel Tower is in Rome. / Paris is a wonderful city to visit. Note The focus is not on the factual accuracy of the sentences but on their consistency or inconsistency with each other.\nOctober 21, 2023: Week 3 Milestones and Tasks Last Week Recap Deployed the survey designed in earlier weeks. Discussed related works. Conducted a pre-study on annotations. This Week’s Milestone: Methods for Data Collection and Annotation Guidelines Goals Finalize methods for data collection. Update Annotation Guidelines based on feedback and discussions. Pilot the new annotation guidelines. Slides Slides for this week will encapsulate the above goals. Updated Annotation Guidelines Changes Video Tutorial: A video was made to explain the new guidelines.\nInstructions: The task now involves reading a passage of text and labeling a claim about that passage.\nClassification: The task has two parts:\nClassifying the passage as supporting, contradicting, or neutral towards the claim. Highlighting sentences that support or contradict the claim (if applicable). Examples: New examples were provided to illustrate what supporting, contradicting, and neutral passages look like.\nTo-Do for Next Week Show HyperMatrix Annotation Platform. Send reminders to those who haven’t completed the survey. Provide feedback on the annotation proposal. Discuss the analysis plan for the survey. Round Table and Annotations Discussion A round table discussion will be conducted to discuss the new annotation guidelines and the pilot study. October 28, 2023: Week 4 Milestones and Tasks Last Week Recap Conducted an annotation pretest. Krippendorff’s Alpha was measured to be 0.54. This Week’s Milestone: Achieve Higher Agreement Goals Aim for higher agreement among annotators. Discuss discrepancies and plan for improvement. Slides Slides will focus on achieving higher inter-rater reliability and discussing disagreements. Round Table and Disagreement Discussion A round table discussion will be held to openly discuss disagreements among annotations. Annotations will be reviewed on Light Tag. Annotation guidelines will be revised and can be found here. To-Do for Next Week Pretest and Annotation Strategies Conduct another pretest. Based on the pretest results: If Inter-Rater Reliability (IRR) is less than ( \\alpha = 0.7 ), annotations will be split 2 ways. If IRR is high, annotations will be split 4 ways. Handling Missing Context Missing context in annotations was identified as a challenge. Two paths for addressing this: Provide the whole paragraph along with the sentence to be measured. Keep it at sentence pairs with a rule: if you don’t have enough information, label it as neutral. Other Tasks Open the option for open text feedback. Complete 300 annotations. I (Yahya) will look into resources for training DeBERTa V3 on annotations, starting with this course. Glossary and Context Inter-Rater Reliability (IRR) What it is: Inter-Rater Reliability (IRR) is a metric used to measure the degree of agreement among multiple raters or annotators.\nWhat it’s for: In the context of this project, IRR is important for ensuring that the annotations being made by different team members are consistent and reliable. This helps in the validation of the dataset being created.\nKrippendorff’s Alpha What it is: Krippendorff’s Alpha is a statistical measure used to determine the reliability of agreement among multiple coders. It is a more general form of the Cronbach’s alpha and can be applied to any number of coders providing ordinal, interval, or ratio data.\nWhat it’s for: In this project, Krippendorff’s Alpha is used as a specific method to quantify IRR. It helps identify how much of the variation in the data can be attributed to the reliability of the annotators.\nThe Need for Higher Agreement Why it’s important: Achieving higher agreement among annotators is crucial for the validity and reliability of the data being generated. It ensures that the interpretations of the text are consistent, thereby making the conclusions drawn from the data more robust. Overview of the Project’s Technical Progress In the final stages of our annotation accuracy project, significant technical contributions were made to enhance the reliability of our data analysis methods. A pivotal aspect of this project involved the training and fine-tuning of the DeBERTa V3 model, which played a crucial role in improving our understanding of annotator agreement and classification accuracy.\nDeep Dive into DeBERTa Model Training Utilizing the DeBERTa (Decoding-enhanced BERT with Disentangled Attention) model, I refined our annotation model. The model was trained on a meticulously prepared dataset where class proportions were balanced based on prior analysis:\nSupports: 54.5752% Contradicts: 24.4009% Neutral: 21.0240% This distribution ensured that our model could effectively learn from a balanced set of examples, reducing bias and improving the generalizability of the model.\nModel Performance and Enhancements Post-training, the DeBERTa model exhibited a substantial increase in inter-rater reliability, achieving an Inter-Rater Reliability (IRR) score well above our initial benchmarks. This was a significant improvement from earlier models and set a new standard for our project’s annotation accuracy.\nPublication and Recognition The results of this research and model training were submitted to a NAACL 2024 conference. I am pleased to report that our paper was accepted for publication, which marks a significant achievement for the team and underscores the quality and importance of our work.\nThe published paper can be accessed here: Long-form evaluation of model editing\n","wordCount":"1600","inLanguage":"en","image":"http://yahyakiani.github.io/blog.yahyakiani.dev/%3Cimage%20path/url%3E","datePublished":"2023-09-23T15:30:03Z","dateModified":"2023-09-23T15:30:03Z","author":{"@type":"Person","name":"Yahya Kayani"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://yahyakiani.github.io/blog.yahyakiani.dev/docs/long_form_editing_paper/"},"publisher":{"@type":"Organization","name":"Yahya Kayani","logo":{"@type":"ImageObject","url":"http://yahyakiani.github.io/blog.yahyakiani.dev/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/ accesskey=h title="Home (Alt + H)"><img src=http://yahyakiani.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/ title=Home><span>Home</span></a></li><li><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/about/ title=About><span>About</span></a></li><li><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/experience/ title=Experience><span>Experience</span></a></li><li><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/projects/ title=Projects><span>Projects</span></a></li><li><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/skills/ title=Skills><span>Skills</span></a></li><li><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/blog/ title=Blog><span>Blog</span></a></li><li><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/resume/ title=Résumé><span>Résumé</span></a></li><li><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/research/ title="Research Papers"><span>Research Papers</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/>Home</a>&nbsp;»&nbsp;<a href=http://yahyakiani.github.io/blog.yahyakiani.dev/docs/>Docs</a></div><h1 class=post-title>Research Progress: Evaluating Model Editing for Natural Language Generation</h1><div class=post-description>Starting on a new research to evaluate model editing in natural language generation.</div><div class=post-meta><span title='2023-09-23 15:30:03 +0000 UTC'>Sept , 23233</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1600 words&nbsp;·&nbsp;Yahya Kayani&nbsp;|&nbsp;<a href=https://github.com/domenicrosati/longform_edit_model_evals/docs/long_form_editing_paper.md rel="noopener noreferrer" target=_blank>Github Profile</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#september-23-2023-embarking-on-model-editing-evaluation><strong>September 23, 2023: Embarking on Model Editing Evaluation</strong></a><ul><li><a href=#research-leadership><strong>Research Leadership</strong></a></li><li><a href=#research-objective><strong>Research Objective</strong></a></li><li><a href=#collaborative-effort><strong>Collaborative Effort</strong></a></li><li><a href=#initial-steps><strong>Initial Steps</strong></a></li><li><a href=#resources--readings><strong>Resources & Readings</strong></a></li></ul></li><li><a href=#october-7-2023-week-1-milestones-and-tasks><strong>October 7, 2023: Week 1 Milestones and Tasks</strong></a><ul><li><a href=#this-weeks-milestone-introduction><strong>This Week&rsquo;s Milestone: Introduction</strong></a></li></ul></li><li><a href=#october-14-2023-week-2-milestones-and-tasks><strong>October 14, 2023: Week 2 Milestones and Tasks</strong></a><ul><li><a href=#last-week-recap><strong>Last Week Recap</strong></a></li><li><a href=#this-weeks-milestone-related-works-and-survey-deployment><strong>This Week&rsquo;s Milestone: Related Works and Survey Deployment</strong></a></li><li><a href=#to-do-for-next-week-1><strong>To-Do for Next Week</strong></a></li><li><a href=#annotation-session-and-feedback><strong>Annotation Session and Feedback</strong></a></li><li><a href=#annotation-instructions><strong>Annotation Instructions</strong></a></li></ul></li><li><a href=#october-21-2023-week-3-milestones-and-tasks><strong>October 21, 2023: Week 3 Milestones and Tasks</strong></a><ul><li><a href=#last-week-recap-1><strong>Last Week Recap</strong></a></li><li><a href=#this-weeks-milestone-methods-for-data-collection-and-annotation-guidelines><strong>This Week&rsquo;s Milestone: Methods for Data Collection and Annotation Guidelines</strong></a></li><li><a href=#updated-annotation-guidelines><strong>Updated Annotation Guidelines</strong></a></li><li><a href=#to-do-for-next-week-2><strong>To-Do for Next Week</strong></a></li></ul></li><li><a href=#october-28-2023-week-4-milestones-and-tasks><strong>October 28, 2023: Week 4 Milestones and Tasks</strong></a><ul><li><a href=#last-week-recap-2><strong>Last Week Recap</strong></a></li><li><a href=#this-weeks-milestone-achieve-higher-agreement><strong>This Week&rsquo;s Milestone: Achieve Higher Agreement</strong></a></li><li><a href=#round-table-and-disagreement-discussion><strong>Round Table and Disagreement Discussion</strong></a></li><li><a href=#to-do-for-next-week-3><strong>To-Do for Next Week</strong></a></li></ul></li><li><a href=#glossary-and-context><strong>Glossary and Context</strong></a><ul><li><a href=#inter-rater-reliability-irr><strong>Inter-Rater Reliability (IRR)</strong></a></li><li><a href=#krippendorffs-alpha><strong>Krippendorff&rsquo;s Alpha</strong></a></li><li><a href=#the-need-for-higher-agreement><strong>The Need for Higher Agreement</strong></a></li></ul></li></ul><ul><li><a href=#deep-dive-into-deberta-model-training>Deep Dive into DeBERTa Model Training</a></li><li><a href=#model-performance-and-enhancements>Model Performance and Enhancements</a></li><li><a href=#publication-and-recognition>Publication and Recognition</a></li></ul></nav></div></details></div><div class=post-content><h2 id=september-23-2023-embarking-on-model-editing-evaluation><strong>September 23, 2023: Embarking on Model Editing Evaluation</strong><a hidden class=anchor aria-hidden=true href=#september-23-2023-embarking-on-model-editing-evaluation>#</a></h2><h3 id=research-leadership><strong>Research Leadership</strong><a hidden class=anchor aria-hidden=true href=#research-leadership>#</a></h3><p>This research is spearheaded by <a href=https://github.com/domenicrosati>Domenic Rosati</a>. Domenic is pivotal in bringing this project to completion and bringing the team up to speed. Collaborating closely with him on this initiative are Yahya(Me), Melis, and Deepika.</p><h3 id=research-objective><strong>Research Objective</strong><a hidden class=anchor aria-hidden=true href=#research-objective>#</a></h3><p>The primary aim of this project is to establish a framework for assessing paragraph-length generations from edited large language models. As the capabilities of large language models grow, so does the importance of understanding their behavior, especially when edited.</p><h3 id=collaborative-effort><strong>Collaborative Effort</strong><a hidden class=anchor aria-hidden=true href=#collaborative-effort>#</a></h3><p>This research is a collaborative initiative involving:</p><ul><li>Domenic, Yahya, Deepika, Melis.</li><li>Supervisors: Lizbeth Escobedo, Hassan Sajjad, Frank Rudzicz</li><li>External Collaborators: Jason (from Counterfact+ dataset), Andrew</li></ul><p>Relevant Resources:</p><ul><li><a href=https://www.zotero.org/groups/5187924/long-form-model-edit-eval>Zotero Group</a></li><li><a href=https://www.overleaf.com/4737549179znvrxghfcyqg>Overleaf Paper Draft</a></li><li><a href=https://github.com/domenicrosati/longform_edit_model_evals>Github Repository</a></li></ul><h4 id=weekly-meeting-time><strong>Weekly Meeting Time</strong><a hidden class=anchor aria-hidden=true href=#weekly-meeting-time>#</a></h4><p>Our team has set a weekly meeting every Wednesday from 10am to 11am AST to discuss progress, issues, and next steps.</p><h3 id=initial-steps><strong>Initial Steps</strong><a hidden class=anchor aria-hidden=true href=#initial-steps>#</a></h3><p>Our first task was to design a 10-passage survey aimed at discerning various properties and values related to model editing. The design specifics and outcomes of this survey will be detailed in the next update.</p><h3 id=resources--readings><strong>Resources & Readings</strong><a hidden class=anchor aria-hidden=true href=#resources--readings>#</a></h3><p>As we dive into this project, several readings and resources have been suggested by Domenic to lay a basic understanding of the subject matter:</p><ul><li>Basics: An understanding of neural networks, transformers, tokens, and more.</li><li>Papers: From understanding the risks posed by language models to evaluating the ripple effects of knowledge editing.</li></ul><h2 id=october-7-2023-week-1-milestones-and-tasks><strong>October 7, 2023: Week 1 Milestones and Tasks</strong><a hidden class=anchor aria-hidden=true href=#october-7-2023-week-1-milestones-and-tasks>#</a></h2><h3 id=this-weeks-milestone-introduction><strong>This Week&rsquo;s Milestone: Introduction</strong><a hidden class=anchor aria-hidden=true href=#this-weeks-milestone-introduction>#</a></h3><h4 id=goals><strong>Goals</strong><a hidden class=anchor aria-hidden=true href=#goals>#</a></h4><ul><li>Understand the overall project and its objectives.</li><li>Familiarize ourselves with the background necessary for writing the introduction section.</li><li>Frame an introduction for our paper.</li><li>Pick a motivation paper to read for next week.</li><li>Read and understand the editing survey we have designed.</li></ul><h4 id=slides><strong>Slides</strong><a hidden class=anchor aria-hidden=true href=#slides>#</a></h4><ul><li>Slides for this week will focus on the above-mentioned goals.</li></ul><h4 id=to-do-for-next-week><strong>To-Do for Next Week</strong><a hidden class=anchor aria-hidden=true href=#to-do-for-next-week>#</a></h4><h5 id=pick-motivation-paper-to-look-at><strong>Pick Motivation Paper to Look At</strong><a hidden class=anchor aria-hidden=true href=#pick-motivation-paper-to-look-at>#</a></h5><p>We have identified several papers that can serve as motivation for our work:</p><ul><li>Petroni, F., et al. (2019). &ldquo;Language models as knowledge bases?&rdquo;.</li><li>Jiang, Z., et al. (2020). &ldquo;How can we know what language models know?&rdquo;.</li><li>Elazar, Y., et al. (2021). &ldquo;Measuring and improving consistency in pretrained language models&rdquo;.</li><li>Lin, S., et al. (2021). &ldquo;Truthfulqa: Measuring how models mimic human falsehoods&rdquo;.</li><li>Gehman, S., et al. (2020). &ldquo;Realtoxicityprompts: Evaluating neural toxic degeneration in language models&rdquo;.</li><li>Bender, E. M., et al. (2021). &ldquo;On the dangers of stochastic parrots: Can language models be too big?&rdquo;.</li><li>Weidinger, L., et al. (2022). &ldquo;Taxonomy of Risks posed by Language Models&rdquo;.</li></ul><h5 id=all-of-us-read><strong>All of Us Read</strong><a hidden class=anchor aria-hidden=true href=#all-of-us-read>#</a></h5><ul><li>Yao, Y., et al. (2023). &ldquo;Editing Large Language Models: Problems, Methods, and Opportunities&rdquo;.</li><li>Visit the webpage: <a href=https://rome.baulab.info/>Rome Lab</a></li><li>Read Appendix J and Counterfactual AI Writing Study in <a href=https://arxiv.org/pdf/2202.05262.pdf>Arxiv Paper</a></li></ul><h2 id=october-14-2023-week-2-milestones-and-tasks><strong>October 14, 2023: Week 2 Milestones and Tasks</strong><a hidden class=anchor aria-hidden=true href=#october-14-2023-week-2-milestones-and-tasks>#</a></h2><h3 id=last-week-recap><strong>Last Week Recap</strong><a hidden class=anchor aria-hidden=true href=#last-week-recap>#</a></h3><h4 id=questions-and-feedback><strong>Questions and Feedback</strong><a hidden class=anchor aria-hidden=true href=#questions-and-feedback>#</a></h4><ul><li>Addressed questions about model editing.</li><li>Discussed pre-test feedback, including issues with paragraph length and clarity of terminologies used for annotation.</li></ul><h3 id=this-weeks-milestone-related-works-and-survey-deployment><strong>This Week&rsquo;s Milestone: Related Works and Survey Deployment</strong><a hidden class=anchor aria-hidden=true href=#this-weeks-milestone-related-works-and-survey-deployment>#</a></h3><h4 id=goals-1><strong>Goals</strong><a hidden class=anchor aria-hidden=true href=#goals-1>#</a></h4><ul><li>Delve into related works, focusing on model editing and evaluation of model editing.</li><li>Deploy the survey designed in Week 1.</li></ul><h4 id=slides-1><strong>Slides</strong><a hidden class=anchor aria-hidden=true href=#slides-1>#</a></h4><ul><li>Slides for this week will center on the above goals.</li></ul><h3 id=to-do-for-next-week-1><strong>To-Do for Next Week</strong><a hidden class=anchor aria-hidden=true href=#to-do-for-next-week-1>#</a></h3><h4 id=annotation-project><strong>Annotation Project</strong><a hidden class=anchor aria-hidden=true href=#annotation-project>#</a></h4><ul><li><p><strong>Reading (All - Optional):</strong></p><ul><li>Papers on annotation best practices and quality management.</li></ul></li><li><p><strong>Brainstorm by Freeform Annotating:</strong></p><ul><li>Group 1 (Dom)</li><li>Group 2 (Deepika)</li><li>Group 3 (Yahya)</li><li>Group 4 (Melis)</li></ul></li><li><p><strong>Other Tasks:</strong></p><ul><li>Develop a draft of Annotation Guidelines (Dom)</li><li>Select a Platform for Annotations (Dom)</li><li>Identify a Dataset for Annotation (Dom)</li></ul></li></ul><h3 id=annotation-session-and-feedback><strong>Annotation Session and Feedback</strong><a hidden class=anchor aria-hidden=true href=#annotation-session-and-feedback>#</a></h3><h4 id=session-overview><strong>Session Overview</strong><a hidden class=anchor aria-hidden=true href=#session-overview>#</a></h4><ul><li>All team members participated in an annotation session.</li><li>We followed the annotation guidelines to label pairs of sentences as consistent, inconsistent, or neutral.</li></ul><h4 id=issues-and-feedback><strong>Issues and Feedback</strong><a hidden class=anchor aria-hidden=true href=#issues-and-feedback>#</a></h4><ul><li>The paragraphs for annotation were found to be too long, making the task cumbersome.</li><li>Some terminologies used in the annotation guidelines were confusing.</li></ul><h4 id=action-items-for-next-annotation-session><strong>Action Items for Next Annotation Session</strong><a hidden class=anchor aria-hidden=true href=#action-items-for-next-annotation-session>#</a></h4><ul><li>Shorten the paragraphs to be annotated.</li><li>Clarify terminologies and guidelines for annotation.</li></ul><h3 id=annotation-instructions><strong>Annotation Instructions</strong><a hidden class=anchor aria-hidden=true href=#annotation-instructions>#</a></h3><h4 id=task-description><strong>Task Description</strong><a hidden class=anchor aria-hidden=true href=#task-description>#</a></h4><p>In this task, you will read pairs of sentences and label them as consistent, inconsistent, or neutral.</p><h4 id=guidelines><strong>Guidelines</strong><a hidden class=anchor aria-hidden=true href=#guidelines>#</a></h4><ul><li><strong>Consistent Sentence Pair</strong>: A pair of sentences that agree with each other.<ul><li><strong>Example</strong>: The Eiffel Tower is in Rome. / Rome is home to the world famous Eiffel Tower.</li></ul></li><li><strong>Inconsistent Sentence Pair</strong>: A pair of sentences that contradict each other.<ul><li><strong>Example</strong>: The Eiffel Tower is in Rome. / Paris is home to the world famous Eiffel Tower.</li></ul></li><li><strong>Neutral Sentence Pair</strong>: A pair of sentences that is neither consistent nor inconsistent.<ul><li><strong>Example</strong>: The Eiffel Tower is in Rome. / Paris is a wonderful city to visit.</li></ul></li></ul><h4 id=note><strong>Note</strong><a hidden class=anchor aria-hidden=true href=#note>#</a></h4><p>The focus is not on the factual accuracy of the sentences but on their consistency or inconsistency with each other.</p><h2 id=october-21-2023-week-3-milestones-and-tasks><strong>October 21, 2023: Week 3 Milestones and Tasks</strong><a hidden class=anchor aria-hidden=true href=#october-21-2023-week-3-milestones-and-tasks>#</a></h2><h3 id=last-week-recap-1><strong>Last Week Recap</strong><a hidden class=anchor aria-hidden=true href=#last-week-recap-1>#</a></h3><ul><li>Deployed the survey designed in earlier weeks.</li><li>Discussed related works.</li><li>Conducted a pre-study on annotations.</li></ul><h3 id=this-weeks-milestone-methods-for-data-collection-and-annotation-guidelines><strong>This Week&rsquo;s Milestone: Methods for Data Collection and Annotation Guidelines</strong><a hidden class=anchor aria-hidden=true href=#this-weeks-milestone-methods-for-data-collection-and-annotation-guidelines>#</a></h3><h4 id=goals-2><strong>Goals</strong><a hidden class=anchor aria-hidden=true href=#goals-2>#</a></h4><ul><li>Finalize methods for data collection.</li><li>Update Annotation Guidelines based on feedback and discussions.</li><li>Pilot the new annotation guidelines.</li></ul><h4 id=slides-2><strong>Slides</strong><a hidden class=anchor aria-hidden=true href=#slides-2>#</a></h4><ul><li>Slides for this week will encapsulate the above goals.</li></ul><h3 id=updated-annotation-guidelines><strong>Updated Annotation Guidelines</strong><a hidden class=anchor aria-hidden=true href=#updated-annotation-guidelines>#</a></h3><h4 id=changes><strong>Changes</strong><a hidden class=anchor aria-hidden=true href=#changes>#</a></h4><ul><li><p><strong>Video Tutorial</strong>: A <a href="https://www.loom.com/share/acb1231f2b774e60b8503e87654adfc5?sid=4959f30b-d350-4f2a-85b5-073b7d70f613">video</a> was made to explain the new guidelines.</p></li><li><p><strong>Instructions</strong>: The task now involves reading a passage of text and labeling a claim about that passage.</p></li><li><p><strong>Classification</strong>: The task has two parts:</p><ol><li>Classifying the passage as supporting, contradicting, or neutral towards the claim.</li><li>Highlighting sentences that support or contradict the claim (if applicable).</li></ol></li><li><p><strong>Examples</strong>: New examples were provided to illustrate what supporting, contradicting, and neutral passages look like.</p></li></ul><h3 id=to-do-for-next-week-2><strong>To-Do for Next Week</strong><a hidden class=anchor aria-hidden=true href=#to-do-for-next-week-2>#</a></h3><ul><li>Show <a href=https://hypermatrix.lighttag.io/label>HyperMatrix Annotation Platform</a>.</li><li>Send reminders to those who haven&rsquo;t completed the survey.</li><li>Provide feedback on the annotation proposal.</li><li>Discuss the analysis plan for the survey.</li></ul><h4 id=round-table-and-annotations-discussion><strong>Round Table and Annotations Discussion</strong><a hidden class=anchor aria-hidden=true href=#round-table-and-annotations-discussion>#</a></h4><ul><li>A round table discussion will be conducted to discuss the new annotation guidelines and the pilot study.</li></ul><h2 id=october-28-2023-week-4-milestones-and-tasks><strong>October 28, 2023: Week 4 Milestones and Tasks</strong><a hidden class=anchor aria-hidden=true href=#october-28-2023-week-4-milestones-and-tasks>#</a></h2><h3 id=last-week-recap-2><strong>Last Week Recap</strong><a hidden class=anchor aria-hidden=true href=#last-week-recap-2>#</a></h3><ul><li>Conducted an annotation pretest.</li><li>Krippendorff&rsquo;s Alpha was measured to be 0.54.</li></ul><h3 id=this-weeks-milestone-achieve-higher-agreement><strong>This Week&rsquo;s Milestone: Achieve Higher Agreement</strong><a hidden class=anchor aria-hidden=true href=#this-weeks-milestone-achieve-higher-agreement>#</a></h3><h4 id=goals-3><strong>Goals</strong><a hidden class=anchor aria-hidden=true href=#goals-3>#</a></h4><ul><li>Aim for higher agreement among annotators.</li><li>Discuss discrepancies and plan for improvement.</li></ul><h4 id=slides-3><strong>Slides</strong><a hidden class=anchor aria-hidden=true href=#slides-3>#</a></h4><ul><li>Slides will focus on achieving higher inter-rater reliability and discussing disagreements.</li></ul><h3 id=round-table-and-disagreement-discussion><strong>Round Table and Disagreement Discussion</strong><a hidden class=anchor aria-hidden=true href=#round-table-and-disagreement-discussion>#</a></h3><ul><li>A round table discussion will be held to openly discuss disagreements among annotations.</li><li>Annotations will be reviewed on <a href=https://lighttag.io/>Light Tag</a>.</li><li>Annotation guidelines will be revised and can be found <a href=https://docs.google.com/document/d/1Ar1X7FZa5obS_vhL556VY9IJXq3cwoY_MwtBYufj8ww/edit>here</a>.</li></ul><h3 id=to-do-for-next-week-3><strong>To-Do for Next Week</strong><a hidden class=anchor aria-hidden=true href=#to-do-for-next-week-3>#</a></h3><h4 id=pretest-and-annotation-strategies><strong>Pretest and Annotation Strategies</strong><a hidden class=anchor aria-hidden=true href=#pretest-and-annotation-strategies>#</a></h4><ul><li>Conduct another pretest.</li><li>Based on the pretest results:<ul><li>If Inter-Rater Reliability (IRR) is less than ( \alpha = 0.7 ), annotations will be split 2 ways.</li><li>If IRR is high, annotations will be split 4 ways.</li></ul></li></ul><h4 id=handling-missing-context><strong>Handling Missing Context</strong><a hidden class=anchor aria-hidden=true href=#handling-missing-context>#</a></h4><ul><li>Missing context in annotations was identified as a challenge.</li><li>Two paths for addressing this:<ol><li>Provide the whole paragraph along with the sentence to be measured.</li><li>Keep it at sentence pairs with a rule: if you don’t have enough information, label it as neutral.</li></ol></li></ul><h4 id=other-tasks><strong>Other Tasks</strong><a hidden class=anchor aria-hidden=true href=#other-tasks>#</a></h4><ul><li>Open the option for open text feedback.</li><li>Complete 300 annotations.</li><li>I (Yahya) will look into resources for training DeBERTa V3 on annotations, starting with <a href="https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt">this course</a>.</li></ul><h2 id=glossary-and-context><strong>Glossary and Context</strong><a hidden class=anchor aria-hidden=true href=#glossary-and-context>#</a></h2><h3 id=inter-rater-reliability-irr><strong>Inter-Rater Reliability (IRR)</strong><a hidden class=anchor aria-hidden=true href=#inter-rater-reliability-irr>#</a></h3><ul><li><p><strong>What it is</strong>: Inter-Rater Reliability (IRR) is a metric used to measure the degree of agreement among multiple raters or annotators.</p></li><li><p><strong>What it&rsquo;s for</strong>: In the context of this project, IRR is important for ensuring that the annotations being made by different team members are consistent and reliable. This helps in the validation of the dataset being created.</p></li></ul><h3 id=krippendorffs-alpha><strong>Krippendorff&rsquo;s Alpha</strong><a hidden class=anchor aria-hidden=true href=#krippendorffs-alpha>#</a></h3><ul><li><p><strong>What it is</strong>: Krippendorff&rsquo;s Alpha is a statistical measure used to determine the reliability of agreement among multiple coders. It is a more general form of the Cronbach&rsquo;s alpha and can be applied to any number of coders providing ordinal, interval, or ratio data.</p></li><li><p><strong>What it&rsquo;s for</strong>: In this project, Krippendorff&rsquo;s Alpha is used as a specific method to quantify IRR. It helps identify how much of the variation in the data can be attributed to the reliability of the annotators.</p></li></ul><h3 id=the-need-for-higher-agreement><strong>The Need for Higher Agreement</strong><a hidden class=anchor aria-hidden=true href=#the-need-for-higher-agreement>#</a></h3><ul><li><strong>Why it&rsquo;s important</strong>: Achieving higher agreement among annotators is crucial for the validity and reliability of the data being generated. It ensures that the interpretations of the text are consistent, thereby making the conclusions drawn from the data more robust.</li></ul><h1 id=overview-of-the-projects-technical-progress>Overview of the Project&rsquo;s Technical Progress<a hidden class=anchor aria-hidden=true href=#overview-of-the-projects-technical-progress>#</a></h1><p>In the final stages of our annotation accuracy project, significant technical contributions were made to enhance the reliability of our data analysis methods. A pivotal aspect of this project involved the training and fine-tuning of the DeBERTa V3 model, which played a crucial role in improving our understanding of annotator agreement and classification accuracy.</p><h2 id=deep-dive-into-deberta-model-training>Deep Dive into DeBERTa Model Training<a hidden class=anchor aria-hidden=true href=#deep-dive-into-deberta-model-training>#</a></h2><p>Utilizing the DeBERTa (Decoding-enhanced BERT with Disentangled Attention) model, I refined our annotation model. The model was trained on a meticulously prepared dataset where class proportions were balanced based on prior analysis:</p><ul><li>Supports: 54.5752%</li><li>Contradicts: 24.4009%</li><li>Neutral: 21.0240%</li></ul><p>This distribution ensured that our model could effectively learn from a balanced set of examples, reducing bias and improving the generalizability of the model.</p><h2 id=model-performance-and-enhancements>Model Performance and Enhancements<a hidden class=anchor aria-hidden=true href=#model-performance-and-enhancements>#</a></h2><p>Post-training, the DeBERTa model exhibited a substantial increase in inter-rater reliability, achieving an Inter-Rater Reliability (IRR) score well above our initial benchmarks. This was a significant improvement from earlier models and set a new standard for our project&rsquo;s annotation accuracy.</p><h2 id=publication-and-recognition>Publication and Recognition<a hidden class=anchor aria-hidden=true href=#publication-and-recognition>#</a></h2><p>The results of this research and model training were submitted to a NAACL 2024 conference. I am pleased to report that our paper was accepted for publication, which marks a significant achievement for the team and underscores the quality and importance of our work.</p><p>The published paper can be accessed here: <a href=https://arxiv.org/abs/2402.09394>Long-form evaluation of model editing</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/tags/nlp/>NLP</a></li><li><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/tags/model-editing/>Model Editing</a></li><li><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/tags/natural-language-generation/>Natural Language Generation</a></li><li><a href=http://yahyakiani.github.io/blog.yahyakiani.dev/tags/research-update/>Research Update</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Research Progress: Evaluating Model Editing for Natural Language Generation on twitter" href="https://twitter.com/intent/tweet/?text=Research%20Progress%3a%20Evaluating%20Model%20Editing%20for%20Natural%20Language%20Generation&url=http%3a%2f%2fyahyakiani.github.io%2fblog.yahyakiani.dev%2fdocs%2flong_form_editing_paper%2f&hashtags=NLP%2cModelEditing%2cNaturalLanguageGeneration%2cResearchUpdate"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Research Progress: Evaluating Model Editing for Natural Language Generation on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=http%3a%2f%2fyahyakiani.github.io%2fblog.yahyakiani.dev%2fdocs%2flong_form_editing_paper%2f&title=Research%20Progress%3a%20Evaluating%20Model%20Editing%20for%20Natural%20Language%20Generation&summary=Research%20Progress%3a%20Evaluating%20Model%20Editing%20for%20Natural%20Language%20Generation&source=http%3a%2f%2fyahyakiani.github.io%2fblog.yahyakiani.dev%2fdocs%2flong_form_editing_paper%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Research Progress: Evaluating Model Editing for Natural Language Generation on reddit" href="https://reddit.com/submit?url=http%3a%2f%2fyahyakiani.github.io%2fblog.yahyakiani.dev%2fdocs%2flong_form_editing_paper%2f&title=Research%20Progress%3a%20Evaluating%20Model%20Editing%20for%20Natural%20Language%20Generation"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Research Progress: Evaluating Model Editing for Natural Language Generation on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2fyahyakiani.github.io%2fblog.yahyakiani.dev%2fdocs%2flong_form_editing_paper%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Research Progress: Evaluating Model Editing for Natural Language Generation on whatsapp" href="https://api.whatsapp.com/send?text=Research%20Progress%3a%20Evaluating%20Model%20Editing%20for%20Natural%20Language%20Generation%20-%20http%3a%2f%2fyahyakiani.github.io%2fblog.yahyakiani.dev%2fdocs%2flong_form_editing_paper%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Research Progress: Evaluating Model Editing for Natural Language Generation on telegram" href="https://telegram.me/share/url?text=Research%20Progress%3a%20Evaluating%20Model%20Editing%20for%20Natural%20Language%20Generation&url=http%3a%2f%2fyahyakiani.github.io%2fblog.yahyakiani.dev%2fdocs%2flong_form_editing_paper%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Research Progress: Evaluating Model Editing for Natural Language Generation on ycombinator" href="https://news.ycombinator.com/submitlink?t=Research%20Progress%3a%20Evaluating%20Model%20Editing%20for%20Natural%20Language%20Generation&u=http%3a%2f%2fyahyakiani.github.io%2fblog.yahyakiani.dev%2fdocs%2flong_form_editing_paper%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://yahyakiani.github.io/blog.yahyakiani.dev/>Yahya Kayani</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>